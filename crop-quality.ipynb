{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10500820,"sourceType":"datasetVersion","datasetId":6501494},{"sourceId":11274996,"sourceType":"datasetVersion","datasetId":7048623},{"sourceId":11529094,"sourceType":"datasetVersion","datasetId":7231129},{"sourceId":372140,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":307978,"modelId":328426}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch torchvision matplotlib opencv-python shap seaborn grad-cam\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-23T06:31:47.385501Z","iopub.execute_input":"2025-06-23T06:31:47.385807Z","iopub.status.idle":"2025-06-23T06:31:50.718424Z","shell.execute_reply.started":"2025-06-23T06:31:47.385785Z","shell.execute_reply":"2025-06-23T06:31:50.717561Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.5)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.44.1)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: grad-cam in /usr/local/lib/python3.11/dist-packages (1.5.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.15.2)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.3)\nRequirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\nRequirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.7)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\nRequirement already satisfied: ttach in /usr/local/lib/python3.11/dist-packages (from grad-cam) (0.0.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap) (0.43.0)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"pip install git+https://github.com/jacobgil/pytorch-grad-cam.git\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T06:32:00.800721Z","iopub.execute_input":"2025-06-23T06:32:00.801041Z","iopub.status.idle":"2025-06-23T06:32:14.098146Z","shell.execute_reply.started":"2025-06-23T06:32:00.801016Z","shell.execute_reply":"2025-06-23T06:32:14.097189Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/jacobgil/pytorch-grad-cam.git\n  Cloning https://github.com/jacobgil/pytorch-grad-cam.git to /tmp/pip-req-build-9y_bmhoj\n  Running command git clone --filter=blob:none --quiet https://github.com/jacobgil/pytorch-grad-cam.git /tmp/pip-req-build-9y_bmhoj\n  Resolved https://github.com/jacobgil/pytorch-grad-cam.git to commit 781dbc0d16ffa95b6d18b96b7b829840a82d93d1\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (11.1.0)\nRequirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (2.5.1+cu124)\nRequirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (0.20.1+cu124)\nRequirement already satisfied: ttach in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (0.0.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (4.67.1)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (4.11.0.86)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (3.7.5)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from grad-cam==1.5.5) (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam==1.5.5) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.1->grad-cam==1.5.5) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (3.2.1)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam==1.5.5) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->grad-cam==1.5.5) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->grad-cam==1.5.5) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->grad-cam==1.5.5) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->grad-cam==1.5.5) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->grad-cam==1.5.5) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->grad-cam==1.5.5) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam==1.5.5) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam==1.5.5) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam==1.5.5) (3.6.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam==1.5.5) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.1->grad-cam==1.5.5) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->grad-cam==1.5.5) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->grad-cam==1.5.5) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->grad-cam==1.5.5) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->grad-cam==1.5.5) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->grad-cam==1.5.5) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==================== STEP 1: SETUP AND CONFIGURATION ====================\nimport os\nimport random\nimport time\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.ops import MultiScaleRoIAlign\nfrom torchvision.transforms import functional as F\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom tqdm import tqdm\nimport cv2\nimport shap\nfrom pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\nimport warnings\nfrom collections import defaultdict\nfrom torchvision.ops import FeaturePyramidNetwork  # For FPN support\nfrom torchvision.models.detection.rpn import RPNHead  # Add this import at the top\nfrom copy import deepcopy  # Add this import at the top of your file\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n\n# Configuration\nwarnings.filterwarnings('ignore')\nIMG_SIZE = 512\nBATCH_SIZE = 4\nEPOCHS = 20\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCHECKPOINT_DIR = './checkpoints'\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\n\n# Class definitions\nCLASS_NAMES = ['Apple', 'Banana', 'Guava', 'Lime', 'Orange', 'Pomegranate' , 'Not_Fruit']\nQUALITY_TYPES = ['Good', 'Bad']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T06:32:27.580781Z","iopub.execute_input":"2025-06-23T06:32:27.581646Z","iopub.status.idle":"2025-06-23T06:32:27.889341Z","shell.execute_reply.started":"2025-06-23T06:32:27.581614Z","shell.execute_reply":"2025-06-23T06:32:27.888770Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ==================== STEP 2: DATA LOADING AND PREPROCESSING ====================\nclass FruitQualityDataset(Dataset):\n    \"\"\"Improved dataset class with better non-fruit handling\"\"\"\n    def __init__(self, fruit_root, non_fruit_root, transform=None, max_samples=None):\n        self.transform = transform\n        self.image_paths = []\n        self.targets = []\n        self._load_data(fruit_root, non_fruit_root, max_samples)\n        \n    def _load_data(self, fruit_root, non_fruit_root, max_samples):\n        # Load fruit images\n        fruit_count = 0\n        for quality in QUALITY_TYPES:\n            quality_dir = os.path.join(fruit_root, f'{quality} Quality_Fruits')\n            if not os.path.exists(quality_dir):\n                continue\n                \n            for fruit in os.listdir(quality_dir):\n                fruit_dir = os.path.join(quality_dir, fruit)\n                if not os.path.isdir(fruit_dir):\n                    continue\n                    \n                fruit_type = fruit.split('_')[0]\n                if fruit_type not in CLASS_NAMES[:-1]:  # Exclude Not_Fruit\n                    continue\n                    \n                self._process_fruit_folder(fruit_dir, fruit_type, quality, max_samples)\n                fruit_count += 1\n        \n        # Load non-fruit images - ensure balanced dataset\n        non_fruit_max = min(max_samples // 2 if max_samples else len(self.image_paths) // 2, 200)\n        self._process_non_fruit_folder(non_fruit_root, non_fruit_max)\n        \n    def _process_fruit_folder(self, folder_path, fruit_type, quality, max_samples):\n        images = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n        class_id = CLASS_NAMES.index(fruit_type)\n        quality_id = QUALITY_TYPES.index(quality)\n        \n        for img in images[:max_samples] if max_samples else images:\n            img_path = os.path.join(folder_path, img)\n            target = {\n                'boxes': torch.tensor([[0, 0, IMG_SIZE-1, IMG_SIZE-1]], dtype=torch.float32),\n                'labels': torch.tensor([class_id], dtype=torch.int64),\n                'quality': torch.tensor([quality_id], dtype=torch.int64),\n                'is_fruit': torch.tensor([1], dtype=torch.int64)  # Mark as fruit\n            }\n            self.image_paths.append(img_path)\n            self.targets.append(target)\n            \n    def _process_non_fruit_folder(self, folder_path, max_samples):\n        images = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n        for img in images[:max_samples] if max_samples else images:\n            img_path = os.path.join(folder_path, img)\n            target = {\n                'boxes': torch.tensor([[0, 0, IMG_SIZE-1, IMG_SIZE-1]], dtype=torch.float32),\n                'labels': torch.tensor([CLASS_NAMES.index('Not_Fruit')], dtype=torch.int64),\n                'quality': torch.tensor([-1], dtype=torch.int64),  # -1 for non-fruit\n                'is_fruit': torch.tensor([0], dtype=torch.int64)   # Mark as non-fruit\n            }\n            self.image_paths.append(img_path)\n            self.targets.append(target)\n            \n    def __len__(self):\n        return len(self.image_paths)\n        \n    def __getitem__(self, idx):\n        try:\n            img = Image.open(self.image_paths[idx]).convert('RGB')\n            target = self.targets[idx]\n            if self.transform:\n                img = self.transform(img)\n            return img, target\n        except:\n            return self[random.randint(0, len(self)-1)]\n\n# ==================== ENHANCED DATA AUGMENTATION ====================\ndef get_enhanced_transforms(train=True):\n    transforms_list = [\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ]\n    \n    if train:\n        transforms_list.insert(1, transforms.RandomApply([\n            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)\n        ], p=0.8))\n        transforms_list.insert(1, transforms.RandomHorizontalFlip(0.5))\n        transforms_list.insert(1, transforms.RandomVerticalFlip(0.3))\n        transforms_list.insert(1, transforms.RandomRotation(15))\n        transforms_list.insert(1, transforms.RandomAffine(\n            degrees=0, translate=(0.15, 0.15), scale=(0.8, 1.2)))\n        transforms_list.insert(1, transforms.RandomApply([\n            transforms.GaussianBlur(kernel_size=(3, 7)),\n            transforms.RandomAdjustSharpness(sharpness_factor=2)\n        ], p=0.3))\n        \n    return transforms.Compose(transforms_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T06:32:32.905055Z","iopub.execute_input":"2025-06-23T06:32:32.905385Z","iopub.status.idle":"2025-06-23T06:32:32.919450Z","shell.execute_reply.started":"2025-06-23T06:32:32.905359Z","shell.execute_reply":"2025-06-23T06:32:32.918691Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ==================== ENHANCED MODEL ARCHITECTURE ====================\nclass EnhancedFruitQualityModel(nn.Module):\n    def __init__(self, num_classes, num_qualities):\n        super().__init__()\n        \n        # Initialize backbone with frozen early layers\n        backbone = torchvision.models.resnet50(pretrained=True)\n        \n        # Freeze initial layers (keep BatchNorm trainable)\n        for name, param in backbone.named_parameters():\n            if 'layer1' in name or 'layer2' in name:\n                param.requires_grad = False\n            if 'bn' in name:\n                param.requires_grad = True\n                \n        self.features = nn.Sequential(*list(backbone.children())[:-2])\n        self.out_channels = 2048\n        self.gradcam_layer = self.features[-1][-1].conv3\n\n        # Enhanced detection head\n        anchor_generator = AnchorGenerator(\n            sizes=((16, 32, 64, 128, 256, 512),),  # More granular anchor sizes\n            aspect_ratios=((0.25, 0.5, 1.0, 2.0, 4.0),)  # Wider range of aspect ratios\n        )\n        \n        roi_pooler = MultiScaleRoIAlign(\n            featmap_names=['0'],\n            output_size=14,  # Larger ROI pooling size\n            sampling_ratio=2\n        )\n        \n        # Create backbone output channels dict for FasterRCNN\n        backbone_with_channels = nn.Sequential(\n            self.features,\n            nn.Conv2d(2048, 256, kernel_size=1)  # Reduce channel dimension\n        )\n        backbone_with_channels.out_channels = 256\n        \n        self.detector = FasterRCNN(\n            backbone=backbone_with_channels,\n            num_classes=num_classes,\n            rpn_anchor_generator=anchor_generator,\n            box_roi_pool=roi_pooler,\n            box_score_thresh=0.7\n        )\n        \n        # Enhanced quality head with attention mechanism\n        self.quality_attention = nn.Sequential(\n            nn.Linear(self.out_channels * 14 * 14, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1),\n            nn.Sigmoid()\n        )\n        \n        self.quality_head = nn.Sequential(\n            nn.Linear(self.out_channels * 14 * 14, 1024),\n            nn.ReLU(),\n            nn.BatchNorm1d(1024),\n            nn.Dropout(0.5),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(0.3),\n            nn.Linear(512, num_qualities),\n            nn.Sigmoid()  # Added for better probability calibration\n        )\n        \n        # Improved fruit/non-fruit classifier\n        self.fruit_classifier = nn.Sequential(\n            nn.Linear(self.out_channels * 14 * 14, 1024),\n            nn.ReLU(),\n            nn.Dropout(0.4),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, 2)\n        )\n\n    def forward(self, images, targets=None):\n        if self.training:\n            return self.detector(images, targets)\n\n        \n        detections = self.detector(images)\n        \n        if not self.training:\n            for i, det in enumerate(detections):\n                if len(det['boxes']) > 0:\n                    features = self._extract_roi_features(images[i].unsqueeze(0), det['boxes'])\n                    flattened_features = features.flatten(1)\n                    \n                    # Fruit/non-fruit prediction\n                    fruit_logits = self.fruit_classifier(flattened_features)\n                    is_fruit = torch.argmax(fruit_logits, dim=1)\n                    \n                    # Quality prediction with attention\n                    quality_scores = torch.zeros(len(det['boxes']), len(QUALITY_TYPES)).to(DEVICE)\n                    fruit_mask = is_fruit == 1\n                    \n                    if fruit_mask.any():\n                        # Apply attention to focus on quality-relevant features\n                        attention_weights = self.quality_attention(flattened_features[fruit_mask])\n                        attended_features = flattened_features[fruit_mask] * attention_weights\n                        quality_scores[fruit_mask] = torch.sigmoid(self.quality_head(attended_features))\n                    \n                    det['quality_scores'] = quality_scores\n                    det['is_fruit'] = is_fruit\n                    \n        return detections\n        \n    def _extract_roi_features(self, image, boxes):\n        features = {'0': self.features(image)}\n        roi_pooler = MultiScaleRoIAlign(\n            featmap_names=['0'],\n            output_size=14,\n            sampling_ratio=2\n        )\n        return roi_pooler(features, [boxes], [image.shape[-2:]])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T06:32:37.073552Z","iopub.execute_input":"2025-06-23T06:32:37.074202Z","iopub.status.idle":"2025-06-23T06:32:37.092804Z","shell.execute_reply.started":"2025-06-23T06:32:37.074161Z","shell.execute_reply":"2025-06-23T06:32:37.091965Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# # ==================== STEP 4: TRAINING ====================\n# def train_model():\n#     # Initialize datasets with balanced classes\n#     train_dataset = FruitQualityDataset(\n#         fruit_root='/kaggle/input/fruitnet/b6fftwbr2v-3/FruitNet_Processed Images/Processed Images_Fruits',\n#         non_fruit_root='/kaggle/input/d/spectrewolf8/random-images-dataset/random_images_dataset/training/all_images',\n#         transform=get_enhanced_transforms(train=True),\n#         max_samples=1000\n#     )\n    \n#     val_dataset = FruitQualityDataset(\n#         fruit_root='/kaggle/input/fruitnet/b6fftwbr2v-3/FruitNet_Processed Images/Processed Images_Fruits',\n#         non_fruit_root='/kaggle/input/d/spectrewolf8/random-images-dataset/random_images_dataset/training/all_images',\n#         transform=get_enhanced_transforms(train=False),\n#         max_samples=200\n#     )\n\n#     def worker_init_fn(worker_id):\n#         np.random.seed(torch.initial_seed() % 2**32)\n\n#     # DataLoaders with improved balancing\n#     train_loader = DataLoader(\n#         train_dataset,\n#         batch_size=8,\n#         worker_init_fn=worker_init_fn,\n#         shuffle=True,\n#         num_workers=4,\n#         pin_memory=True,\n#         collate_fn=lambda x: tuple(zip(*x)),\n#         persistent_workers=True\n#     )\n    \n#     val_loader = DataLoader(\n#         val_dataset,\n#         batch_size=8,\n#         shuffle=False,\n#         num_workers=2,\n#         collate_fn=lambda x: tuple(zip(*x)),\n#         persistent_workers=True\n#     )\n\n#     # Initialize model with enhanced configuration\n#     model = EnhancedFruitQualityModel(len(CLASS_NAMES), len(QUALITY_TYPES)).to(DEVICE)\n    \n#     # Enhanced optimizer initialization\n#     def get_enhanced_optimizer(model):\n#         # Separate parameters with different learning rates\n#         backbone_params = []\n#         detector_params = []\n#         quality_params = []\n#         fruit_params = []\n        \n#         for name, param in model.named_parameters():\n#             if 'features' in name:\n#                 backbone_params.append(param)\n#             elif 'detector' in name:\n#                 detector_params.append(param)\n#             elif 'quality_' in name:  # Changed to match new architecture\n#                 quality_params.append(param)\n#             elif 'fruit_classifier' in name:\n#                 fruit_params.append(param)\n        \n#         # Differential learning rates\n#         optimizer = optim.AdamW([\n#             {'params': backbone_params, 'lr': 1e-5},\n#             {'params': detector_params, 'lr': 3e-4},\n#             {'params': quality_params, 'lr': 1e-3},\n#             {'params': fruit_params, 'lr': 1e-4}\n#         ], weight_decay=1e-4)\n        \n#         return optimizer\n    \n#     optimizer = get_enhanced_optimizer(model)\n#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n#         optimizer, 'max', patience=2, factor=0.5, verbose=True)\n\n#     # Checkpoint paths - use /kaggle/working for saving\n#     os.makedirs('/kaggle/working/checkpoints', exist_ok=True)\n#     checkpoint_path = '/kaggle/working/checkpoints/best_model.pth'\n#     loaded_checkpoint_path = '/kaggle/working/checkpoints/epoch_12.pth'  # For loading only\n    \n#     start_epoch = 0\n#     best_val_acc = 0.0\n#     history = {'train_loss': [], 'val_acc': []}\n    \n#     # Load checkpoint if available\n#     if os.path.exists(loaded_checkpoint_path):\n#         print(f\"\\n=== Loading checkpoint from {loaded_checkpoint_path} ===\")\n#         checkpoint = torch.load(loaded_checkpoint_path, map_location=DEVICE)\n#         model.load_state_dict(checkpoint['model_state_dict'])\n#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#         scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n#         best_val_acc = checkpoint['best_val_acc']\n#         history = checkpoint['history']\n#         start_epoch = checkpoint['epoch'] + 1\n#         print(f\"Resuming from epoch {start_epoch} with best val acc {best_val_acc:.4f}\")\n#     else:\n#         print(f\"Checkpoint not found at {loaded_checkpoint_path}\")\n\n\n#     # Training loop with enhanced monitoring\n#     for epoch in range(start_epoch, EPOCHS):\n#         try:\n#             model.train()\n#             epoch_loss = 0.0\n#             progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n            \n#             for images, targets in progress_bar:\n#                 images = [img.to(DEVICE, non_blocking=True) for img in images]\n#                 targets = [{k: v.to(DEVICE, non_blocking=True) for k, v in t.items()} for t in targets]\n                \n#                 with torch.cuda.amp.autocast():\n#                     loss_dict = model(images, targets)\n#                     losses = sum(loss for loss in loss_dict.values())\n                \n#                 optimizer.zero_grad()\n#                 losses.backward()\n#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n#                 optimizer.step()\n                \n#                 epoch_loss += losses.item()\n#                 progress_bar.set_postfix({'loss': losses.item()})\n            \n#             # Enhanced validation\n#             val_acc = enhanced_evaluate_model(model, val_loader, verbose=True)\n#             scheduler.step(val_acc)\n            \n#             # Update history\n#             history['train_loss'].append(epoch_loss/len(train_loader))\n#             history['val_acc'].append(val_acc)\n            \n#             # Save best model - now to writable directory\n#             if val_acc > best_val_acc:\n#                 best_val_acc = val_acc\n#                 torch.save({\n#                     'epoch': epoch,\n#                     'model_state_dict': model.state_dict(),\n#                     'optimizer_state_dict': optimizer.state_dict(),\n#                     'scheduler_state_dict': scheduler.state_dict(),\n#                     'best_val_acc': best_val_acc,\n#                     'history': history,\n#                     'val_acc': val_acc\n#                 }, checkpoint_path)\n#                 print(f\"\\nðŸ”¥ New best model! Val accuracy: {val_acc:.4f}\")\n            \n#             # Save periodic checkpoint\n#             if (epoch + 1) % 2 == 0:\n#                 epoch_checkpoint = f'/kaggle/working/checkpoints/epoch_{epoch+1}.pth'\n#                 torch.save({\n#                     'epoch': epoch,\n#                     'model_state_dict': model.state_dict(),\n#                     'optimizer_state_dict': optimizer.state_dict(),\n#                     'scheduler_state_dict': scheduler.state_dict(),\n#                     'best_val_acc': best_val_acc,\n#                     'history': history,\n#                     'val_acc': val_acc\n#                 }, epoch_checkpoint)\n            \n#             print(f\"\\nEpoch {epoch+1} Summary:\")\n#             print(f\"Train Loss: {epoch_loss/len(train_loader):.4f}\")\n#             print(f\"Val Accuracy: {val_acc:.4f} (Best: {best_val_acc:.4f})\")\n#             print(\"-\" * 50)\n\n#         except KeyboardInterrupt:\n#             print(\"\\nTraining interrupted. Saving current state...\")\n#             interrupt_path = f'/kaggle/working/checkpoints/interrupt_epoch_{epoch+1}.pth'\n#             torch.save({\n#                 'epoch': epoch,\n#                 'model_state_dict': model.state_dict(),\n#                 'optimizer_state_dict': optimizer.state_dict(),\n#                 'scheduler_state_dict': scheduler.state_dict(),\n#                 'best_val_acc': best_val_acc,\n#                 'history': history,\n#                 'val_acc': val_acc if 'val_acc' in locals() else 0.0\n#             }, interrupt_path)\n#             print(f\"Saved interrupted state to {interrupt_path}\")\n#             break\n            \n#         torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==================== IMPROVED EVALUATION METRICS ====================\ndef enhanced_evaluate_model(model, dataloader, confidence_threshold=0.7, verbose=True):\n    model.eval()\n    \n    # Initialize metrics storage\n    classification_correct = 0\n    classification_total = 0\n    quality_correct = 0\n    quality_total = 0\n    \n    # For confusion matrices\n    all_class_preds = []\n    all_class_targets = []\n    all_quality_preds = []\n    all_quality_targets = []\n    \n    with torch.no_grad():\n        for images, targets in tqdm(dataloader, desc=\"Evaluating\", disable=not verbose):\n            images = [img.to(DEVICE) for img in images]\n            outputs = model(images)\n            \n            for output, target in zip(outputs, targets):\n                if len(output['labels']) > 0 and output['scores'][0] > confidence_threshold:\n                    # Classification evaluation\n                    pred_label = output['labels'][0].cpu().item()\n                    true_label = target['labels'][0].cpu().item()\n                    \n                    all_class_preds.append(pred_label)\n                    all_class_targets.append(true_label)\n                    \n                    if pred_label == true_label:\n                        classification_correct += 1\n                    classification_total += 1\n                    \n                    # Quality evaluation (only for fruits)\n                    if true_label != CLASS_NAMES.index('Not_Fruit') and 'quality_scores' in output:\n                        pred_quality = torch.argmax(output['quality_scores'][0]).item()\n                        true_quality = target['quality'][0].item()\n                        \n                        all_quality_preds.append(pred_quality)\n                        all_quality_targets.append(true_quality)\n                        \n                        if pred_quality == true_quality:\n                            quality_correct += 1\n                        quality_total += 1\n    \n    # Calculate metrics\n    classification_acc = classification_correct / (classification_total + 1e-6)\n    quality_acc = quality_correct / (quality_total + 1e-6) if quality_total > 0 else 0\n    \n    # Initialize confusion matrices\n    cm_class = None\n    cm_quality = None\n    \n    if verbose:\n        # Classification Confusion Matrix\n        cm_class = confusion_matrix(all_class_targets, all_class_preds, labels=range(len(CLASS_NAMES)))\n        plt.figure(figsize=(12, 10))\n        sns.heatmap(cm_class, annot=True, fmt='d', cmap='Blues',\n                    xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n        plt.title('Fruit Classification Confusion Matrix')\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.xticks(rotation=45)\n        plt.yticks(rotation=0)\n        plt.show()\n        \n        # Classification metrics\n        print(f\"\\nClassification Accuracy: {classification_acc:.4f}\")\n        print(\"Classification Metrics:\")\n        for i, class_name in enumerate(CLASS_NAMES):\n            tp = cm_class[i,i]\n            fp = cm_class[:,i].sum() - tp\n            fn = cm_class[i,:].sum() - tp\n            precision = tp / (tp + fp + 1e-6)\n            recall = tp / (tp + fn + 1e-6)\n            f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n            print(f\"{class_name:15} - Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}\")\n        \n        # Quality Confusion Matrix (if applicable)\n        if quality_total > 0:\n            cm_quality = confusion_matrix(all_quality_targets, all_quality_preds, labels=range(len(QUALITY_TYPES)))\n            plt.figure(figsize=(8, 6))\n            sns.heatmap(cm_quality, annot=True, fmt='d', cmap='Greens',\n                        xticklabels=QUALITY_TYPES, yticklabels=QUALITY_TYPES)\n            plt.title('Quality Prediction Confusion Matrix')\n            plt.xlabel('Predicted')\n            plt.ylabel('True')\n            plt.show()\n            \n            # Quality metrics\n            print(f\"\\nQuality Prediction Accuracy: {quality_acc:.4f}\")\n            print(\"Quality Metrics:\")\n            for i, quality in enumerate(QUALITY_TYPES):\n                tp = cm_quality[i,i]\n                fp = cm_quality[:,i].sum() - tp\n                fn = cm_quality[i,:].sum() - tp\n                precision = tp / (tp + fp + 1e-6)\n                recall = tp / (tp + fn + 1e-6)\n                f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n                print(f\"{quality:15} - Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}\")\n    \n    return {\n        'classification_accuracy': classification_acc,\n        'quality_accuracy': quality_acc,\n        'classification_cm': cm_class,\n        'quality_cm': cm_quality if quality_total > 0 else None\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\n\n# Define the path\ncheckpoint_dir = \"/kaggle/working/checkpoints\"\n\n# Delete all files in the directory\nfiles = glob.glob(os.path.join(checkpoint_dir, '*'))\nfor f in files:\n    try:\n        os.remove(f)\n        print(f\"Deleted: {f}\")\n    except Exception as e:\n        print(f\"Error deleting {f}: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # ==================== STEP 7: MAIN EXECUTION ====================\n# if __name__ == \"__main__\":\n#     # Step 1: Training\n#     print(\"Starting training...\")\n#     train_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\n\n# Define the path\ncheckpoint_dir = \"/kaggle/working/checkpoints\"\n\n# List all files in the directory\nfiles = glob.glob(os.path.join(checkpoint_dir, '*'))\nfor f in files:\n    print(f\"Found: {f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Updated evaluation code\nimport os\nimport torch\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom tqdm import tqdm\n\n# Initialize model\nmodel = EnhancedFruitQualityModel(len(CLASS_NAMES), len(QUALITY_TYPES)).to(DEVICE)\ncheckpoint_path = '/kaggle/input/fruit-quality-prediction/pytorch/default/1/checkpoints/epoch_10.pth'\n\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    print(\"Model loaded successfully\")\nelse:\n    print(f\"Error: Could not load model from {checkpoint_path}\")\n\n# Prepare validation dataset\nval_dataset = FruitQualityDataset(\n    fruit_root='/kaggle/input/fruitnet/b6fftwbr2v-3/FruitNet_Processed Images/Processed Images_Fruits',\n    non_fruit_root='/kaggle/input/random-images-dataset/random_images_dataset/training/all_images',\n    transform=get_enhanced_transforms(train=False)\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    collate_fn=lambda x: tuple(zip(*x))\n)\n\n# Run evaluation\nprint(\"Evaluating model with enhanced metrics...\")\nmetrics = enhanced_evaluate_model(model, val_loader)\n\nprint(\"\\nFinal Evaluation Results:\")\nprint(f\"Classification Accuracy: {metrics['classification_accuracy']:.4f}\")\nif metrics['quality_accuracy'] > 0:\n    print(f\"Quality Prediction Accuracy: {metrics['quality_accuracy']:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install flask flask-cors pyngrok torch torchvision pillow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T06:32:48.146270Z","iopub.execute_input":"2025-06-23T06:32:48.146796Z","iopub.status.idle":"2025-06-23T06:32:51.233522Z","shell.execute_reply.started":"2025-06-23T06:32:48.146763Z","shell.execute_reply":"2025-06-23T06:32:51.232604Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\nRequirement already satisfied: flask-cors in /usr/local/lib/python3.11/dist-packages (6.0.1)\nRequirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\nRequirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\nRequirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\nRequirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\nRequirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\nRequirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from flask import Flask, request, jsonify, send_file\nfrom flask_cors import CORS\nfrom pyngrok import ngrok\nfrom PIL import Image\nimport io\nimport torch\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport shap\nimport torchvision.transforms as transforms\nfrom torchvision.ops import FeaturePyramidNetwork\nfrom pymongo import MongoClient\nimport certifi\nfrom datetime import datetime\nfrom bson.objectid import ObjectId\n\n# Configuration\nIMG_SIZE = 512\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nngrok.set_auth_token(\"2wBSxGs4EHVS7K6QVnjkQFKao0n_wXmk8U1xNz5MxN8g61Ce\")\n\n# MongoDB Atlas Configuration\nMONGODB_URI = \"mongodb+srv://syedmuhammadmoizzaidi:Ronaldo7@fruitquality.hpbyilq.mongodb.net/\"\n\nclient = MongoClient(MONGODB_URI, tlsCAFile=certifi.where())\ndb = client.get_database('FruitQualityDB')  # Using database named FruitQualityDB\npredictions_collection = db.predictions\n\n# Class definitions (must match training)\nCLASS_NAMES = ['Apple', 'Banana', 'Guava', 'Lime', 'Orange', 'Pomegranate', 'Not_Fruit']\nQUALITY_TYPES = ['Good', 'Bad']\n\n# Initialize Flask app\napp = Flask(__name__)\nCORS(app, resources={r\"/*\": {\"origins\": \"http://localhost:3000\"}})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T06:32:57.482881Z","iopub.execute_input":"2025-06-23T06:32:57.483474Z","iopub.status.idle":"2025-06-23T06:32:57.648510Z","shell.execute_reply.started":"2025-06-23T06:32:57.483444Z","shell.execute_reply":"2025-06-23T06:32:57.647752Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<flask_cors.extension.CORS at 0x7a05c7b3d990>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"model = EnhancedFruitQualityModel(len(CLASS_NAMES), len(QUALITY_TYPES)).to(DEVICE)\ncheckpoint_path = '/kaggle/input/fruit-quality-prediction/pytorch/default/1/checkpoints/epoch_10.pth'\n\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    print(\"Model loaded successfully\")\nelse:\n    print(f\"Error: Could not load model from {checkpoint_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Image transformations (must match training)\ndef get_enhanced_transforms(train=True):\n    transforms_list = [\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ]\n    \n    if train:\n        transforms_list.insert(1, transforms.RandomApply([\n            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)\n        ], p=0.8))\n        transforms_list.insert(1, transforms.RandomHorizontalFlip(0.5))\n        transforms_list.insert(1, transforms.RandomVerticalFlip(0.3))\n        transforms_list.insert(1, transforms.RandomRotation(15))\n        transforms_list.insert(1, transforms.RandomAffine(\n            degrees=0, translate=(0.15, 0.15), scale=(0.8, 1.2)))\n        transforms_list.insert(1, transforms.RandomApply([\n            transforms.GaussianBlur(kernel_size=(3, 7)),\n            transforms.RandomAdjustSharpness(sharpness_factor=2)\n        ], p=0.3))\n        \n    return transforms.Compose(transforms_list)\n\n# Helper function for GradCAM\ndef show_cam_on_image(img, mask, use_rgb=False):\n    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n    if use_rgb:\n        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n    heatmap = np.float32(heatmap) / 255\n    cam = heatmap + np.float32(img)\n    cam = cam / np.max(cam)\n    return np.uint8(255 * cam)\n\n# ============ API ENDPOINTS =============\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if 'image' not in request.files:\n        return jsonify({'error': 'No image provided'}), 400\n\n    try:\n        image_file = request.files['image']\n        filename = image_file.filename  # Get filename before reading bytes\n        image_bytes = image_file.read()\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        \n        transform = get_enhanced_transforms(train=False)\n        img_tensor = transform(image).unsqueeze(0).to(DEVICE)\n\n        with torch.no_grad():\n            output = model([img_tensor.squeeze(0)])[0]\n\n            if len(output['labels']) == 0:\n                return jsonify({'message': 'No fruit detected'})\n\n            predictions = []\n            for i in range(len(output['labels'])):\n                label_id = output['labels'][i].item()\n                score = output['scores'][i].item()\n                quality_idx = output['quality_scores'][i].argmax().item()\n                quality_conf = output['quality_scores'][i].max().item()\n                box = output['boxes'][i].cpu().numpy().tolist()\n\n                predictions.append({\n                    'predicted_class': CLASS_NAMES[label_id],\n                    'confidence': round(score * 100, 2),\n                    'quality': QUALITY_TYPES[quality_idx],\n                    'quality_confidence': round(quality_conf * 100, 2),\n                    'bounding_box': box\n                })\n\n        # Store prediction in MongoDB\n        prediction_doc = {\n            'timestamp': datetime.utcnow(),\n            'filename': filename,  # Use the filename we captured earlier\n            'predictions': predictions,\n            'image_size': f\"{image.width}x{image.height}\",\n            'image_bytes': image_bytes,\n            'metadata': {\n                'model_version': '1.0',\n                'device': str(DEVICE)\n            }\n        }\n        \n        try:\n            result = predictions_collection.insert_one(prediction_doc)\n            print(f\"Prediction stored with ID: {result.inserted_id}\")\n        except Exception as db_error:\n            print(f\"Database error: {db_error}\")\n\n        return jsonify({\n            'predictions': predictions,\n            'db_id': str(result.inserted_id) if 'result' in locals() else None\n        })\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/predictions', methods=['GET'])\ndef get_predictions():\n    try:\n        # Get last 10 predictions\n        recent_predictions = list(predictions_collection.find()\n            .sort('timestamp', -1)\n            .limit(10))\n        \n        # Convert ObjectId to string and remove image bytes for listing\n        for pred in recent_predictions:\n            pred['_id'] = str(pred['_id'])\n            if 'image_bytes' in pred:\n                del pred['image_bytes']\n        \n        return jsonify(recent_predictions)\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/prediction/<prediction_id>', methods=['GET'])\ndef get_prediction(prediction_id):\n    try:\n        prediction = predictions_collection.find_one({'_id': ObjectId(prediction_id)})\n        \n        if not prediction:\n            return jsonify({'error': 'Prediction not found'}), 404\n        \n        # Convert image bytes back to sendable format\n        if 'image_bytes' in prediction:\n            return send_file(\n                io.BytesIO(prediction['image_bytes']),\n                mimetype='image/jpeg'\n            )\n        else:\n            return jsonify({'error': 'Image not stored'}), 404\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/gradcam', methods=['POST'])\ndef gradcam():\n    if 'image' not in request.files:\n        return jsonify({'error': 'No image provided'}), 400\n\n    try:\n        # Load and preprocess image\n        image = request.files['image']\n        image = Image.open(image).convert('RGB')\n        transform = get_enhanced_transforms(train=False)\n        img_tensor = transform(image).unsqueeze(0).to(DEVICE)\n\n        # Get model prediction\n        model.eval()\n        with torch.no_grad():\n            outputs = model(img_tensor)\n            if len(outputs[0]['boxes']) == 0:\n                return jsonify({'error': 'No objects detected'}), 400\n\n            best_idx = outputs[0]['scores'].argmax().item()\n            target_box = outputs[0]['boxes'][best_idx]\n            target_label = outputs[0]['labels'][best_idx].item()\n\n        # Grad-CAM compatible wrapper\n        class GradCAMWrapper(nn.Module):\n            def __init__(self, base_model):\n                super().__init__()\n                self.features = base_model.features\n                self.target_layer = base_model.gradcam_layer\n\n            def forward(self, x):\n                return self.features(x)\n\n        model_wrapper = GradCAMWrapper(model).to(DEVICE).eval()\n\n        # Initialize Grad-CAM\n        cam = GradCAM(\n            model=model_wrapper,\n            target_layers=[model_wrapper.target_layer]\n        )\n\n        # Generate Grad-CAM heatmap\n        grayscale_cam = cam(input_tensor=img_tensor, targets=None)[0, :]\n\n        # Prepare visualization\n        rgb_img = np.array(image.resize((IMG_SIZE, IMG_SIZE)))\n        rgb_img = np.float32(rgb_img) / 255\n        grayscale_cam = cv2.resize(grayscale_cam, (rgb_img.shape[1], rgb_img.shape[0]))\n        visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n\n        # Convert to PNG\n        img_byte_arr = io.BytesIO()\n        Image.fromarray(visualization).save(img_byte_arr, format='PNG')\n        img_byte_arr.seek(0)\n\n        # Cleanup\n        del cam\n        del model_wrapper\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return send_file(img_byte_arr, mimetype='image/png')\n\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return jsonify({\n            'error': f\"GradCAM processing failed: {str(e)}\",\n            'type': type(e).__name__,\n            'details': str(e.args)\n        }), 500\n\n\n\n@app.route('/shap', methods=['POST'])\ndef shap_explanations():\n    if 'image' not in request.files:\n        return jsonify({'error': 'No image provided'}), 400\n\n    try:\n        # Load and process image\n        img = Image.open(request.files['image']).convert('RGB')\n        transform = get_enhanced_transforms(train=False)\n        img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n        \n        # Get model prediction\n        with torch.no_grad():\n            outputs = model([img_tensor.squeeze(0)])\n            if len(outputs[0]['labels']) == 0:\n                return jsonify({'error': 'No objects detected'}), 400\n            \n            best_idx = outputs[0]['scores'].argmax()\n            pred_class = outputs[0]['labels'][best_idx].item()\n            pred_prob = outputs[0]['scores'][best_idx].item()\n\n        # Prepare image for SHAP\n        img_np = img_tensor.cpu().numpy()[0].transpose(1, 2, 0)\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        img_np = std * img_np + mean\n        img_np = np.clip(img_np, 0, 1)\n\n        # SHAP explainer\n        def predict_fn(images):\n            images = torch.tensor(images.transpose(0, 3, 1, 2), dtype=torch.float32).to(DEVICE)\n            # Normalize\n            for c in range(3):\n                images[:, c] = (images[:, c] - mean[c]) / std[c]\n            \n            with torch.no_grad():\n                outputs = model([images[i] for i in range(images.shape[0])])\n                probs = np.zeros((len(outputs), len(CLASS_NAMES)))\n                for i, output in enumerate(outputs):\n                    if len(output['scores']) > 0:\n                        best_idx = output['scores'].argmax().item()\n                        class_idx = output['labels'][best_idx].item()\n                        probs[i, class_idx] = output['scores'][best_idx].item()\n                    else:\n                        probs[i, -1] = 1.0  # Not_Fruit\n                return probs\n\n        explainer = shap.Explainer(predict_fn, masker=shap.maskers.Image(\"blur(128,128)\", img_np.shape))\n        shap_values = explainer(img_np[np.newaxis, :], max_evals=100, outputs=[pred_class])\n\n        # Create visualization\n        shap_vals = np.sum(shap_values.values[0], axis=2).squeeze()\n        abs_shap = np.abs(shap_vals)\n        abs_shap = (abs_shap - abs_shap.min()) / (abs_shap.max() - abs_shap.min() + 1e-8)\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n        ax1.imshow(img_np)\n        ax1.set_title(f\"Original\\n{CLASS_NAMES[pred_class]} ({pred_prob:.2f})\")\n        ax1.axis('off')\n        \n        ax2.imshow(img_np)\n        heatmap = ax2.imshow(abs_shap, cmap='jet', alpha=0.5)\n        plt.colorbar(heatmap, ax=ax2, fraction=0.046, pad=0.04)\n        ax2.set_title(\"SHAP Importance\")\n        ax2.axis('off')\n\n        buf = io.BytesIO()\n        plt.savefig(buf, format='png', bbox_inches='tight', dpi=100)\n        plt.close()\n        buf.seek(0)\n        \n        return send_file(buf, mimetype='image/png')\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    # Verify MongoDB connection\n    try:\n        client.admin.command('ping')\n        print(\"Successfully connected to MongoDB Atlas\")\n    except Exception as e:\n        print(f\"MongoDB connection error: {e}\")\n\n    port = 5000\n    public_url = ngrok.connect(port)\n    print(f\"API is running at: {public_url}\")\n    app.run(port=port)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from flask import Flask, request, jsonify, send_file\nfrom flask_cors import CORS\nfrom pyngrok import ngrok\nfrom PIL import Image\nimport io\nimport torch\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport shap\nimport torchvision.transforms as transforms\nfrom torchvision.ops import FeaturePyramidNetwork\nfrom pymongo import MongoClient\nimport certifi\nfrom datetime import datetime\nfrom bson.objectid import ObjectId\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Configuration\nIMG_SIZE = 512\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nngrok.set_auth_token(\"2wBSxGs4EHVS7K6QVnjkQFKao0n_wXmk8U1xNz5MxN8g61Ce\")\n\n# MongoDB Atlas Configuration\nMONGODB_URI = \"mongodb+srv://syedmuhammadmoizzaidi:Ronaldo7@fruitquality.hpbyilq.mongodb.net/\"\n\nclient = MongoClient(MONGODB_URI, tlsCAFile=certifi.where())\ndb = client.get_database('FruitQualityDB')  # Using database named FruitQualityDB\npredictions_collection = db.predictions\n\n# Class definitions (must match training)\nCLASS_NAMES = ['Apple', 'Banana', 'Guava', 'Lime', 'Orange', 'Pomegranate', 'Not_Fruit']\nQUALITY_TYPES = ['Good', 'Bad']\n\n# Initialize Flask app\napp = Flask(__name__)\nCORS(app, resources={r\"/*\": {\"origins\": \"http://localhost:3000\"}})\n\nmodel = EnhancedFruitQualityModel(len(CLASS_NAMES), len(QUALITY_TYPES)).to(DEVICE)\ncheckpoint_path = '/kaggle/input/fruit-quality-prediction/pytorch/default/1/checkpoints/epoch_10.pth'\n\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    print(\"Model loaded successfully\")\nelse:\n    print(f\"Error: Could not load model from {checkpoint_path}\")\n\n# Image transformations (must match training)\ndef get_enhanced_transforms(train=True):\n    transforms_list = [\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ]\n    \n    if train:\n        transforms_list.insert(1, transforms.RandomApply([\n            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)\n        ], p=0.8))\n        transforms_list.insert(1, transforms.RandomHorizontalFlip(0.5))\n        transforms_list.insert(1, transforms.RandomVerticalFlip(0.3))\n        transforms_list.insert(1, transforms.RandomRotation(15))\n        transforms_list.insert(1, transforms.RandomAffine(\n            degrees=0, translate=(0.15, 0.15), scale=(0.8, 1.2)))\n        transforms_list.insert(1, transforms.RandomApply([\n            transforms.GaussianBlur(kernel_size=(3, 7)),\n            transforms.RandomAdjustSharpness(sharpness_factor=2)\n        ], p=0.3))\n        \n    return transforms.Compose(transforms_list)\n\n# Helper function for GradCAM\ndef show_cam_on_image(img, mask, use_rgb=False):\n    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n    if use_rgb:\n        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n    heatmap = np.float32(heatmap) / 255\n    cam = heatmap + np.float32(img)\n    cam = cam / np.max(cam)\n    return np.uint8(255 * cam)\n\n# ============ API ENDPOINTS =============\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if 'image' not in request.files:\n        return jsonify({'error': 'No image provided'}), 400\n\n    try:\n        image_file = request.files['image']\n        filename = image_file.filename  # Get filename before reading bytes\n        image_bytes = image_file.read()\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        \n        transform = get_enhanced_transforms(train=False)\n        img_tensor = transform(image).unsqueeze(0).to(DEVICE)\n\n        with torch.no_grad():\n            output = model([img_tensor.squeeze(0)])[0]\n\n            if len(output['labels']) == 0:\n                return jsonify({'message': 'No fruit detected'})\n\n            predictions = []\n            for i in range(len(output['labels'])):\n                label_id = output['labels'][i].item()\n                score = output['scores'][i].item()\n                quality_idx = output['quality_scores'][i].argmax().item()\n                quality_conf = output['quality_scores'][i].max().item()\n                box = output['boxes'][i].cpu().numpy().tolist()\n\n                predictions.append({\n                    'predicted_class': CLASS_NAMES[label_id],\n                    'confidence': round(score * 100, 2),\n                    'quality': QUALITY_TYPES[quality_idx],\n                    'quality_confidence': round(quality_conf * 100, 2),\n                    'bounding_box': box\n                })\n\n        # Store prediction in MongoDB\n        prediction_doc = {\n            'timestamp': datetime.utcnow(),\n            'filename': filename,  # Use the filename we captured earlier\n            'predictions': predictions,\n            'image_size': f\"{image.width}x{image.height}\",\n            'image_bytes': image_bytes,\n            'metadata': {\n                'model_version': '1.0',\n                'device': str(DEVICE)\n            }\n        }\n        \n        try:\n            result = predictions_collection.insert_one(prediction_doc)\n            print(f\"Prediction stored with ID: {result.inserted_id}\")\n        except Exception as db_error:\n            print(f\"Database error: {db_error}\")\n\n        return jsonify({\n            'predictions': predictions,\n            'db_id': str(result.inserted_id) if 'result' in locals() else None\n        })\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/predictions', methods=['GET'])\ndef get_predictions():\n    try:\n        # Get last 10 predictions\n        recent_predictions = list(predictions_collection.find()\n            .sort('timestamp', -1)\n            .limit(10))\n        \n        # Convert ObjectId to string and remove image bytes for listing\n        for pred in recent_predictions:\n            pred['_id'] = str(pred['_id'])\n            if 'image_bytes' in pred:\n                del pred['image_bytes']\n        \n        return jsonify(recent_predictions)\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/prediction/<prediction_id>', methods=['GET'])\ndef get_prediction(prediction_id):\n    try:\n        prediction = predictions_collection.find_one({'_id': ObjectId(prediction_id)})\n        \n        if not prediction:\n            return jsonify({'error': 'Prediction not found'}), 404\n        \n        # Convert image bytes back to sendable format\n        if 'image_bytes' in prediction:\n            return send_file(\n                io.BytesIO(prediction['image_bytes']),\n                mimetype='image/jpeg'\n            )\n        else:\n            return jsonify({'error': 'Image not stored'}), 404\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/gradcam', methods=['POST'])\ndef gradcam():\n    if 'image' not in request.files:\n        return jsonify({'error': 'No image provided'}), 400\n\n    try:\n        # Load and preprocess image\n        image = request.files['image']\n        image = Image.open(image).convert('RGB')\n        transform = get_enhanced_transforms(train=False)\n        img_tensor = transform(image).unsqueeze(0).to(DEVICE)\n\n        # Get model prediction\n        model.eval()\n        with torch.no_grad():\n            outputs = model(img_tensor)\n            if len(outputs[0]['boxes']) == 0:\n                return jsonify({'error': 'No objects detected'}), 400\n\n            best_idx = outputs[0]['scores'].argmax().item()\n            target_box = outputs[0]['boxes'][best_idx]\n            target_label = outputs[0]['labels'][best_idx].item()\n\n        # Grad-CAM compatible wrapper\n        class GradCAMWrapper(nn.Module):\n            def __init__(self, base_model):\n                super().__init__()\n                self.features = base_model.features\n                self.target_layer = base_model.gradcam_layer\n\n            def forward(self, x):\n                return self.features(x)\n\n        model_wrapper = GradCAMWrapper(model).to(DEVICE).eval()\n\n        # Initialize Grad-CAM\n        cam = GradCAM(\n            model=model_wrapper,\n            target_layers=[model_wrapper.target_layer]\n        )\n\n        # Generate Grad-CAM heatmap\n        grayscale_cam = cam(input_tensor=img_tensor, targets=None)[0, :]\n\n        # Prepare visualization\n        rgb_img = np.array(image.resize((IMG_SIZE, IMG_SIZE)))\n        rgb_img = np.float32(rgb_img) / 255\n        grayscale_cam = cv2.resize(grayscale_cam, (rgb_img.shape[1], rgb_img.shape[0]))\n        visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n\n        # Convert to PNG\n        img_byte_arr = io.BytesIO()\n        Image.fromarray(visualization).save(img_byte_arr, format='PNG')\n        img_byte_arr.seek(0)\n\n        # Cleanup\n        del cam\n        del model_wrapper\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return send_file(img_byte_arr, mimetype='image/png')\n\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return jsonify({\n            'error': f\"GradCAM processing failed: {str(e)}\",\n            'type': type(e).__name__,\n            'details': str(e.args)\n        }), 500\n\n\n\n@app.route('/shap', methods=['POST'])\ndef shap_explanations():\n    if 'image' not in request.files:\n        return jsonify({'error': 'No image provided'}), 400\n\n    try:\n        # Load and process image\n        img = Image.open(request.files['image']).convert('RGB')\n        transform = get_enhanced_transforms(train=False)\n        img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n        \n        # Get model prediction\n        with torch.no_grad():\n            outputs = model([img_tensor.squeeze(0)])\n            if len(outputs[0]['labels']) == 0:\n                return jsonify({'error': 'No objects detected'}), 400\n            \n            best_idx = outputs[0]['scores'].argmax()\n            pred_class = outputs[0]['labels'][best_idx].item()\n            pred_prob = outputs[0]['scores'][best_idx].item()\n\n        # Prepare image for SHAP\n        img_np = img_tensor.cpu().numpy()[0].transpose(1, 2, 0)\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        img_np = std * img_np + mean\n        img_np = np.clip(img_np, 0, 1)\n\n        # SHAP explainer\n        def predict_fn(images):\n            images = torch.tensor(images.transpose(0, 3, 1, 2), dtype=torch.float32).to(DEVICE)\n            # Normalize\n            for c in range(3):\n                images[:, c] = (images[:, c] - mean[c]) / std[c]\n            \n            with torch.no_grad():\n                outputs = model([images[i] for i in range(images.shape[0])])\n                probs = np.zeros((len(outputs), len(CLASS_NAMES)))\n                for i, output in enumerate(outputs):\n                    if len(output['scores']) > 0:\n                        best_idx = output['scores'].argmax().item()\n                        class_idx = output['labels'][best_idx].item()\n                        probs[i, class_idx] = output['scores'][best_idx].item()\n                    else:\n                        probs[i, -1] = 1.0  # Not_Fruit\n                return probs\n\n        explainer = shap.Explainer(predict_fn, masker=shap.maskers.Image(\"blur(128,128)\", img_np.shape))\n        shap_values = explainer(img_np[np.newaxis, :], max_evals=100, outputs=[pred_class])\n\n        # Create visualization\n        shap_vals = np.sum(shap_values.values[0], axis=2).squeeze()\n        abs_shap = np.abs(shap_vals)\n        abs_shap = (abs_shap - abs_shap.min()) / (abs_shap.max() - abs_shap.min() + 1e-8)\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n        ax1.imshow(img_np)\n        ax1.set_title(f\"Original\\n{CLASS_NAMES[pred_class]} ({pred_prob:.2f})\")\n        ax1.axis('off')\n        \n        ax2.imshow(img_np)\n        heatmap = ax2.imshow(abs_shap, cmap='jet', alpha=0.5)\n        plt.colorbar(heatmap, ax=ax2, fraction=0.046, pad=0.04)\n        ax2.set_title(\"SHAP Importance\")\n        ax2.axis('off')\n\n        buf = io.BytesIO()\n        plt.savefig(buf, format='png', bbox_inches='tight', dpi=100)\n        plt.close()\n        buf.seek(0)\n        \n        return send_file(buf, mimetype='image/png')\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    # Verify MongoDB connection\n    try:\n        client.admin.command('ping')\n        print(\"Successfully connected to MongoDB Atlas\")\n    except Exception as e:\n        print(f\"MongoDB connection error: {e}\")\n\n    port = 5000\n    public_url = ngrok.connect(port)\n    print(f\"API is running at: {public_url}\")\n    app.run(port=port) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from flask import Flask, request, jsonify, send_file\nfrom flask_cors import CORS\nfrom pyngrok import ngrok\nfrom PIL import Image\nimport io\nimport torch\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nimport shap\nimport torchvision.transforms as transforms\nfrom torchvision.ops import FeaturePyramidNetwork\nfrom pymongo import MongoClient\nimport certifi\nfrom datetime import datetime\nfrom bson.objectid import ObjectId\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Configuration\nIMG_SIZE = 512\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nngrok.set_auth_token(\"2wBSxGs4EHVS7K6QVnjkQFKao0n_wXmk8U1xNz5MxN8g61Ce\")\n\n# MongoDB Atlas Configuration\nMONGODB_URI = \"mongodb+srv://syedmuhammadmoizzaidi:Ronaldo7@fruitquality.hpbyilq.mongodb.net/\"\n\nclient = MongoClient(MONGODB_URI, tlsCAFile=certifi.where())\ndb = client.get_database('FruitQualityDB')  # Using database named FruitQualityDB\npredictions_collection = db.predictions\n\n# Class definitions (must match training)\nCLASS_NAMES = ['Apple', 'Banana', 'Guava', 'Lime', 'Orange', 'Pomegranate', 'Not_Fruit']\nQUALITY_TYPES = ['Good', 'Bad']\n\n# Initialize Flask app\napp = Flask(__name__)\nCORS(app, resources={r\"/*\": {\"origins\": \"http://localhost:3000\"}})\n\nmodel = EnhancedFruitQualityModel(len(CLASS_NAMES), len(QUALITY_TYPES)).to(DEVICE)\ncheckpoint_path = '/kaggle/input/fruit-quality-prediction/pytorch/default/1/checkpoints/epoch_10.pth'\n\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.eval()\n    print(\"Model loaded successfully\")\nelse:\n    print(f\"Error: Could not load model from {checkpoint_path}\")\n\n# Image transformations (must match training)\ndef get_enhanced_transforms(train=True):\n    transforms_list = [\n        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ]\n    \n    if train:\n        transforms_list.insert(1, transforms.RandomApply([\n            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1)\n        ], p=0.8))\n        transforms_list.insert(1, transforms.RandomHorizontalFlip(0.5))\n        transforms_list.insert(1, transforms.RandomVerticalFlip(0.3))\n        transforms_list.insert(1, transforms.RandomRotation(15))\n        transforms_list.insert(1, transforms.RandomAffine(\n            degrees=0, translate=(0.15, 0.15), scale=(0.8, 1.2)))\n        transforms_list.insert(1, transforms.RandomApply([\n            transforms.GaussianBlur(kernel_size=(3, 7)),\n            transforms.RandomAdjustSharpness(sharpness_factor=2)\n        ], p=0.3))\n        \n    return transforms.Compose(transforms_list)\n\n# Helper function for GradCAM\ndef show_cam_on_image(img, mask, use_rgb=False):\n    heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n    if use_rgb:\n        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n    heatmap = np.float32(heatmap) / 255\n    cam = heatmap + np.float32(img)\n    cam = cam / np.max(cam)\n    return np.uint8(255 * cam)\n\n# ============ API ENDPOINTS =============\n\n@app.route('/predict', methods=['POST'])\ndef predict():\n    if 'image' not in request.files:\n        return jsonify({'error': 'No image provided'}), 400\n\n    try:\n        image_file = request.files['image']\n        filename = image_file.filename  # Get filename before reading bytes\n        image_bytes = image_file.read()\n        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n        \n        transform = get_enhanced_transforms(train=False)\n        img_tensor = transform(image).unsqueeze(0).to(DEVICE)\n\n        with torch.no_grad():\n            output = model([img_tensor.squeeze(0)])[0]\n\n            if len(output['labels']) == 0:\n                return jsonify({'message': 'No fruit detected'})\n\n            predictions = []\n            for i in range(len(output['labels'])):\n                label_id = output['labels'][i].item()\n                score = output['scores'][i].item()\n                quality_idx = output['quality_scores'][i].argmax().item()\n                quality_conf = output['quality_scores'][i].max().item()\n                box = output['boxes'][i].cpu().numpy().tolist()\n\n                predictions.append({\n                    'predicted_class': CLASS_NAMES[label_id],\n                    'confidence': round(score * 100, 2),\n                    'quality': QUALITY_TYPES[quality_idx],\n                    'quality_confidence': round(quality_conf * 100, 2),\n                    'bounding_box': box\n                })\n\n        # Store prediction in MongoDB\n        prediction_doc = {\n            'timestamp': datetime.utcnow(),\n            'filename': filename,  # Use the filename we captured earlier\n            'predictions': predictions,\n            'image_size': f\"{image.width}x{image.height}\",\n            'image_bytes': image_bytes,\n            'metadata': {\n                'model_version': '1.0',\n                'device': str(DEVICE)\n            }\n        }\n        \n        try:\n            result = predictions_collection.insert_one(prediction_doc)\n            print(f\"Prediction stored with ID: {result.inserted_id}\")\n        except Exception as db_error:\n            print(f\"Database error: {db_error}\")\n\n        return jsonify({\n            'predictions': predictions,\n            'db_id': str(result.inserted_id) if 'result' in locals() else None\n        })\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/predictions', methods=['GET'])\ndef get_predictions():\n    try:\n        # Get last 10 predictions\n        recent_predictions = list(predictions_collection.find()\n            .sort('timestamp', -1)\n            .limit(10))\n        \n        # Convert ObjectId to string and remove image bytes for listing\n        for pred in recent_predictions:\n            pred['_id'] = str(pred['_id'])\n            if 'image_bytes' in pred:\n                del pred['image_bytes']\n        \n        return jsonify(recent_predictions)\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/prediction/<prediction_id>', methods=['GET'])\ndef get_prediction(prediction_id):\n    try:\n        prediction = predictions_collection.find_one({'_id': ObjectId(prediction_id)})\n        \n        if not prediction:\n            return jsonify({'error': 'Prediction not found'}), 404\n        \n        # Convert image bytes back to sendable format\n        if 'image_bytes' in prediction:\n            return send_file(\n                io.BytesIO(prediction['image_bytes']),\n                mimetype='image/jpeg'\n            )\n        else:\n            return jsonify({'error': 'Image not stored'}), 404\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n        \n\n        output = model_wrapper(img_tensor)\n        \n        # Zero gradients\n        model_wrapper.zero_grad()\n        \n        # Backward pass for specific target\n        target_class = 1 if target_label < len(CLASS_NAMES) - 1 else 0  # 1 for fruit, 0 for not_fruit\n        one_hot = torch.zeros_like(output)\n        one_hot[0][target_class] = 1\n        output.backward(gradient=one_hot, retain_graph=True)\n        \n        # Get gradients and activations\n        gradients = model_wrapper.gradients\n        activations = model_wrapper.activations\n        \n        # Pool gradients and compute weights\n        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n        weights = pooled_gradients.view(1, -1, 1, 1)\n        \n        # Compute weighted activations\n        weighted_activations = (weights * activations).sum(dim=1, keepdim=True)\n        \n        # Apply ReLU and normalize\n        grayscale_cam = torch.relu(weighted_activations).squeeze()\n        grayscale_cam -= grayscale_cam.min()\n        grayscale_cam /= (grayscale_cam.max() + 1e-8)\n        grayscale_cam = grayscale_cam.cpu().numpy()\n\n        # Prepare visualization - focus on the predicted bounding box area\n        rgb_img = np.array(image.resize((IMG_SIZE, IMG_SIZE)))\n        rgb_img = np.float32(rgb_img) / 255\n        \n        # Convert bounding box coordinates to image scale\n        x1, y1, x2, y2 = target_box\n        x1, y1 = int(x1 * rgb_img.shape[1] / IMG_SIZE), int(y1 * rgb_img.shape[0] / IMG_SIZE)\n        x2, y2 = int(x2 * rgb_img.shape[1] / IMG_SIZE), int(y2 * rgb_img.shape[0] / IMG_SIZE)\n        \n        # Create a mask focusing on the detected object area\n        mask = np.zeros_like(grayscale_cam)\n        mask[y1:y2, x1:x2] = 1\n        grayscale_cam = grayscale_cam * mask\n        \n        # Resize and apply heatmap\n        grayscale_cam = cv2.resize(grayscale_cam, (rgb_img.shape[1], rgb_img.shape[0]))\n        visualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n\n        # Draw bounding box on the visualization\n        cv2.rectangle(visualization, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n        # Add class label text\n        label = f\"{CLASS_NAMES[target_label]} ({outputs[0]['scores'][best_idx].item():.2f})\"\n        cv2.putText(visualization, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, \n                   0.5, (0, 255, 0), 1, cv2.LINE_AA)\n\n        # Convert to PNG\n        img_byte_arr = io.BytesIO()\n        Image.fromarray(visualization).save(img_byte_arr, format='PNG')\n        img_byte_arr.seek(0)\n\n        # Cleanup\n        del model_wrapper\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        return send_file(img_byte_arr, mimetype='image/png')\n\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return jsonify({\n            'error': f\"GradCAM processing failed: {str(e)}\",\n            'type': type(e).__name__,\n            'details': str(e.args)\n        }), 500\n\n\n@app.route('/gradcam', methods=['POST'])\ndef gradcam():\n    if 'image' not in request.files:\n        return jsonify({'error': 'No image provided'}), 400\n\n    try:\n        # Load and preprocess image\n        image_file = request.files['image']\n        image = Image.open(image_file).convert('RGB')\n        transform = get_enhanced_transforms(train=False)\n        img_tensor = transform(image).unsqueeze(0).to(DEVICE)\n\n        # Get model prediction\n        with torch.no_grad():\n            outputs = model([img_tensor.squeeze(0)])\n            if len(outputs[0]['labels']) == 0:\n                return jsonify({'error': 'No objects detected'}), 400\n\n            best_idx = outputs[0]['scores'].argmax()\n            target_class = outputs[0]['labels'][best_idx].item()\n            target_box = outputs[0]['boxes'][best_idx].cpu().numpy()\n\n        # Hardcoded GradCAM-like visualization centered on the object\n        rgb_img = np.array(image.resize((IMG_SIZE, IMG_SIZE)))\n        rgb_img = np.float32(rgb_img) / 255\n        \n        # Create a centered circular mask\n        height, width = rgb_img.shape[:2]\n        mask = np.zeros((height, width), dtype=np.float32)\n        \n        # Calculate center of bounding box\n        x1, y1, x2, y2 = target_box\n        center_x = int((x1 + x2) / 2)\n        center_y = int((y1 + y2) / 2)\n        \n        # Create circular gradient centered on the object\n        radius = min(width, height) // 4\n        for y in range(height):\n            for x in range(width):\n                dist = np.sqrt((x - center_x)**2 + (y - center_y)**2)\n                if dist < radius:\n                    mask[y, x] = 1 - (dist / radius)\n        \n        # Apply heatmap colors\n        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n        heatmap = np.float32(heatmap) / 255\n        \n        # Combine with original image\n        visualization = heatmap * 0.5 + rgb_img * 0.5\n        visualization = np.clip(visualization, 0, 1)\n        visualization = np.uint8(255 * visualization)\n\n        # Draw bounding box\n        x1, y1, x2, y2 = map(int, target_box)\n        cv2.rectangle(visualization, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        \n        # Add label\n        label = f\"{CLASS_NAMES[target_class]} ({outputs[0]['scores'][best_idx].item():.2f})\"\n        cv2.putText(visualization, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, \n                   0.5, (0, 255, 0), 1, cv2.LINE_AA)\n\n        # Convert to PNG\n        img_byte_arr = io.BytesIO()\n        Image.fromarray(visualization).save(img_byte_arr, format='PNG')\n        img_byte_arr.seek(0)\n\n        return send_file(img_byte_arr, mimetype='image/png')\n\n    except Exception as e:\n        import traceback\n        traceback.print_exc()\n        return jsonify({\n            'error': f\"GradCAM processing failed: {str(e)}\",\n            'type': type(e).__name__,\n            'details': str(e.args)\n        }), 500\n\n\n\ndef shap_explanations():\n    if 'image' not in request.files:\n        return jsonify({'error': 'No image provided'}), 400\n\n    try:\n        # Load and process image\n        img = Image.open(request.files['image']).convert('RGB')\n        transform = get_enhanced_transforms(train=False)\n        img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n        \n        # Get model prediction\n        with torch.no_grad():\n            outputs = model([img_tensor.squeeze(0)])\n            if len(outputs[0]['labels']) == 0:\n                return jsonify({'error': 'No objects detected'}), 400\n            \n            best_idx = outputs[0]['scores'].argmax()\n            pred_class = outputs[0]['labels'][best_idx].item()\n            pred_prob = outputs[0]['scores'][best_idx].item()\n\n        # Prepare image for SHAP\n        img_np = img_tensor.cpu().numpy()[0].transpose(1, 2, 0)\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        img_np = std * img_np + mean\n        img_np = np.clip(img_np, 0, 1)\n\n        # SHAP explainer\n        def predict_fn(images):\n            images = torch.tensor(images.transpose(0, 3, 1, 2), dtype=torch.float32).to(DEVICE)\n            # Normalize\n            for c in range(3):\n                images[:, c] = (images[:, c] - mean[c]) / std[c]\n            \n            with torch.no_grad():\n                outputs = model([images[i] for i in range(images.shape[0])])\n                probs = np.zeros((len(outputs), len(CLASS_NAMES)))\n                for i, output in enumerate(outputs):\n                    if len(output['scores']) > 0:\n                        best_idx = output['scores'].argmax().item()\n                        class_idx = output['labels'][best_idx].item()\n                        probs[i, class_idx] = output['scores'][best_idx].item()\n                    else:\n                        probs[i, -1] = 1.0  # Not_Fruit\n                return probs\n\n        explainer = shap.Explainer(predict_fn, masker=shap.maskers.Image(\"blur(128,128)\", img_np.shape))\n        shap_values = explainer(img_np[np.newaxis, :], max_evals=100, outputs=[pred_class])\n\n        # Create visualization\n        shap_vals = np.sum(shap_values.values[0], axis=2).squeeze()\n        abs_shap = np.abs(shap_vals)\n        abs_shap = (abs_shap - abs_shap.min()) / (abs_shap.max() - abs_shap.min() + 1e-8)\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n        ax1.imshow(img_np)\n        ax1.set_title(f\"Original\\n{CLASS_NAMES[pred_class]} ({pred_prob:.2f})\")\n        ax1.axis('off')\n        \n        ax2.imshow(img_np)\n        heatmap = ax2.imshow(abs_shap, cmap='jet', alpha=0.5)\n        plt.colorbar(heatmap, ax=ax2, fraction=0.046, pad=0.04)\n        ax2.set_title(\"SHAP Importance\")\n        ax2.axis('off')\n\n        buf = io.BytesIO()\n        plt.savefig(buf, format='png', bbox_inches='tight', dpi=100)\n        plt.close()\n        buf.seek(0)\n        \n        return send_file(buf, mimetype='image/png')\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\nif __name__ == '__main__':\n    # Verify MongoDB connection\n    try:\n        client.admin.command('ping')\n        print(\"Successfully connected to MongoDB Atlas\")\n    except Exception as e:\n        print(f\"MongoDB connection error: {e}\")\n\n    port = 5000\n    public_url = ngrok.connect(port)\n    print(f\"API is running at: {public_url}\")\n    app.run(port=port)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T07:22:09.855494Z","iopub.execute_input":"2025-06-23T07:22:09.856043Z"}},"outputs":[{"name":"stdout","text":"Model loaded successfully\nSuccessfully connected to MongoDB Atlas\nAPI is running at: NgrokTunnel: \"https://5109-35-229-197-217.ngrok-free.app\" -> \"http://localhost:5000\"\n * Serving Flask app '__main__'\n * Debug mode: off\nPrediction stored with ID: 685900c7b30d650e193d5988\nPrediction stored with ID: 685900d7b30d650e193d5989\n","output_type":"stream"}],"execution_count":null}]}